import numpy as np
import matplotlib.pyplot as plt
import numpy as np
import pickle
import gymnasium as gym
import time



def choose_action(state, q_table, epsilon, n_actions):
    """ Îµ-greedy action selection """
    if np.random.rand() < epsilon:
        return np.random.choice(n_actions)
    return np.argmax(q_table[state])


def train_q_learning(env, episodes=10000, alpha=0.8, gamma=0.95,
                     epsilon=1.0, epsilon_decay=0.999, min_epsilon=0.05,
                     max_steps=100):
    n_states = env.observation_space.n
    n_actions = env.action_space.n
    q_table = np.zeros((n_states, n_actions))
    rewards_per_episode = []

    for ep in range(episodes):
        state, _ = env.reset()
        total_reward = 0

        for _ in range(max_steps):
            action = choose_action(state, q_table, epsilon, n_actions)
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            # Q-learning update (off-policy)
            q_table[state, action] += alpha * (
                reward + gamma * np.max(q_table[next_state]) - q_table[state, action]
            )

            state = next_state
            total_reward += reward
            if done:
                break

        epsilon = max(min_epsilon, epsilon * epsilon_decay)
        rewards_per_episode.append(total_reward)

    return q_table, rewards_per_episode



def train_sarsa(env, episodes=10000, alpha=0.8, gamma=0.95,
                epsilon=1.0, epsilon_decay=0.999, min_epsilon=0.05,
                max_steps=100):
    n_states = env.observation_space.n
    n_actions = env.action_space.n
    q_table = np.zeros((n_states, n_actions))
    rewards_per_episode = []

    for ep in range(episodes):
        state, _ = env.reset()
        action = choose_action(state, q_table, epsilon, n_actions)
        total_reward = 0

        for _ in range(max_steps):
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            next_action = choose_action(next_state, q_table, epsilon, n_actions)

            # SARSA update (on-policy)
            q_table[state, action] += alpha * (
                reward + gamma * q_table[next_state, next_action] - q_table[state, action]
            )

            state = next_state
            action = next_action
            total_reward += reward
            if done:
                break

        epsilon = max(min_epsilon, epsilon * epsilon_decay)
        rewards_per_episode.append(total_reward)

    return q_table, rewards_per_episode


def plot_training_curve(rewards, title="Training Progress", window=100):
    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')
    plt.figure(figsize=(10, 5))
    plt.plot(smoothed)
    plt.title(title)
    plt.xlabel("Episode")
    plt.ylabel(f"Average Reward (Window={window})")
    plt.grid(True)
    filename = title.lower().replace(" ", "_") + ".png"
    plt.savefig(filename, bbox_inches='tight')
    plt.show()


def save_q_table(q_table, filename):
    with open(filename, "wb") as f:
        pickle.dump(q_table, f)

def load_q_table(filename):
    with open(filename, "rb") as f:
        return pickle.load(f)
    

import numpy as np

def print_policy_map(q_table, map_layout=None):
    """
    Print the best policy as a 4x4 grid with arrow directions.
    """
    actions = ["â†", "â†“", "â†’", "â†‘"]  # A0, A1, A2, A3
    policy = []

    # frozen Lake layout
    if map_layout is None:
        map_layout = [
            'S', 'F', 'F', 'F',
            'F', 'H', 'F', 'H',
            'F', 'F', 'F', 'H',
            'H', 'F', 'F', 'G'
        ]

    for state in range(len(map_layout)):
        tile = map_layout[state]
        if tile == 'H':
            policy.append('X')  # Hole
        elif tile == 'G':
            policy.append('G')  # Goal
        elif tile == 'S':
            policy.append('S')  # Start
        else:
            best_action = np.argmax(q_table[state])
            policy.append(actions[best_action])

    # Print as 4x4 grid
    policy_grid = np.array(policy).reshape((4, 4))
    print("\nðŸ§­ Learned Policy Map:\n")
    for row in policy_grid:
        print(" ".join(row))


env = gym.make("FrozenLake-v1", is_slippery=False)

# === Q-learning ===
q_table_q, rewards_q = train_q_learning(env)
save_q_table(q_table_q, "q_learning_q_table.pkl")
plot_training_curve(rewards_q, "Q-learning Training Curve")
print("Q-learning Policy:")
print_policy_map(q_table_q)

# === SARSA ===
q_table_sarsa, rewards_sarsa = train_sarsa(env)
save_q_table(q_table_sarsa, "sarsa_q_table.pkl")
plot_training_curve(rewards_sarsa, "SARSA Training Curve")
print("SARSA Policy:")
print_policy_map(q_table_sarsa)




s